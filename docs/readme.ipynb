{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "Welcome to Jupyter!\n",
    "\n",
    "This notebook is designed for demonstrating the capabilities of the [Spatial DS Stack](https://github.com/zacharlie/spatial-ds-stack), which is a simple stack for connecting a bespoke data science environment for exploratory data analysis (EDA).\n",
    "\n",
    "The services made available in the stack include:\n",
    "\n",
    "- Jupyter: The interactive noteboook and coding environment\n",
    "- Python libraries: Such as pandas, seaborn, dask, pycaret, and scikit-learn\n",
    "- PostgreSQL: A local database service\n",
    "- PgAdmin: Database administration frontend \n",
    "- MLflow: A machine learning lifecycle management system\n",
    "\n",
    "A flatfile=>geodata ingestion service is provided alongside the stack to push data from the filesystem into the database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make sure that you have completed the instructions in the README.md file included with this repository to ensure that your notebook environment is properly configured before attempting the following steps.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Database Connection Configuration\n",
    "\n",
    "To interact with the database, we will have to configure connections. Configuration of this step is important due to the security implications of exposing db connection details.\n",
    "\n",
    "One way or another, if you can load the config into a workbook then those config values are probably accessible to anyone using that workbook, most likely in an unencrypted manner.\n",
    "\n",
    "The stack uses the configparser from the python standard library and loading of the config file as a docker secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from configparser import ConfigParser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConfigParser is a python standard module for loading configuration settings from INI formatted sources. By default, your database and environment configuration should include the `/nb.cfg` configuration file which is loaded as a docker secret. \n",
    "\n",
    "> Note that because it is loaded into the Jupyter container, anyone with access to this environment can extract the data from that file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test configuration setup\n",
    "\n",
    "CONFIG = ConfigParser()\n",
    "CONFIG.read('/nb.cfg')\n",
    "print(f\"Your configured database user is {CONFIG['db']['USERNAME']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set environment variables from config\n",
    "LOCAL_POSTGRES_HOST = CONFIG['db']['HOST']\n",
    "LOCAL_POSTGRES_PORT = CONFIG['db']['PORT']\n",
    "LOCAL_POSTGRES_DBNAME = CONFIG['db']['DBNAME']\n",
    "LOCAL_POSTGRES_USERNAME = CONFIG['db']['USERNAME']\n",
    "LOCAL_POSTGRES_PASSWORD = CONFIG['db']['PASSWORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define local connection string for sqlalchemy orm\n",
    "import urllib\n",
    "\n",
    "pg_conn_string_local = ('postgresql://{user}:{pword}@{host}:{port}/{dbname}'\n",
    "                        .format(user=urllib.parse.quote_plus(LOCAL_POSTGRES_USERNAME),\n",
    "                                pword=urllib.parse.quote_plus(LOCAL_POSTGRES_PASSWORD),\n",
    "                                host=LOCAL_POSTGRES_HOST,\n",
    "                                port=LOCAL_POSTGRES_PORT,\n",
    "                                dbname=LOCAL_POSTGRES_DBNAME))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local DB Access with SQLAlchemy\n",
    "\n",
    "SQLAlchemy is a python object relational mapper (ORM) for enabling interactions with SQL databases. We can use this to directly interact with supported databases, such as postgresql, with a set of classes which provide helper methods, input sanitization, and utility classes for database interaction. ORMs are most useful for developers unfamiliar with complex SQL operations, as well as simplifying or abstracting code for developer teams effectively whilst enforcing best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the local connection\n",
    "local_engine = db.create_engine(pg_conn_string_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test the local connection\n",
    "local_inspector = db.inspect(local_engine)\n",
    "local_inspector.get_schema_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view local tables and columns\n",
    "for table_name in local_inspector.get_table_names():\n",
    "    for column in local_inspector.get_columns(table_name):\n",
    "        print(f\"{table_name}.{column['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view local tables in the geodata schema\n",
    "for table_name in local_inspector.get_table_names(schema='geodata'):\n",
    "    print(f\"{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with local_engine.connect() as connection:\n",
    "    query_result = connection.execute(db.text('''\n",
    "SELECT\n",
    "    column_name,\n",
    "    data_type\n",
    "FROM\n",
    "    information_schema.columns\n",
    "WHERE\n",
    "    table_schema = \\'geodata\\'\n",
    "AND\n",
    "    table_name = \\'spatial_ref_sys\\';\n",
    "'''))\n",
    "\n",
    "query_result.fetchall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Access with psycopg2\n",
    "\n",
    "A complex system such as the Data Gateway may not support connecting via an ORM like SQLAlchemy, but we can connect directly to the Data Integration HUB via a postgresql compatible. In this stack, we can use the included `psycopg2` library for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define remote connection for data gateway with psycopg2\n",
    "import psycopg2\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    host = REMOTE_POSTGRES_HOST,\n",
    "    port = REMOTE_POSTGRES_PORT,\n",
    "    dbname = REMOTE_POSTGRES_DBNAME,\n",
    "    user = REMOTE_POSTGRES_USERNAME,\n",
    "    password = REMOTE_POSTGRES_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a connection cursor\n",
    "# (basically an instance of a database control structure)\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cursor.execute('''\n",
    "  , col1\n",
    "  , col2\n",
    "FROM\n",
    "  table1\n",
    "LIMIT\n",
    "  10\n",
    "''')\n",
    "result = cursor.fetchall()\n",
    "# close the connection when completed\n",
    "connection.close()\n",
    "# Note that the retrieved result is still available in memory\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the query data into a data frame\n",
    "df = pd.DataFrame(result)\n",
    "df.columns = ['col1', 'col2']\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval and Exploration\n",
    "\n",
    "Now that we've seen some database connectivity in action, let's have a look at how to retrieve data from the database and analyze it's structure.\n",
    "\n",
    "First we need to access the metadata object for the database to setup the SQLAlchemy context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Metadata Object\n",
    "local_meta = db.MetaData()\n",
    "local_meta.reflect(local_engine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stack includes the `postgis` extension in the local database which creates a `spatial_ref_sys` table. We can query the table and retrieve records from the local database using python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Spatial Reference System table from the Metadata object\n",
    "SRS = local_meta.tables['spatial_ref_sys']\n",
    "\n",
    "# SQLAlchemy Query to select all rows based on some condition\n",
    "query = db.select(SRS).where(SRS.c.srid > 10000)\n",
    "\n",
    "# Fetch the records\n",
    "with local_engine.connect() as cnx:\n",
    "    results = cnx.execute(query)\n",
    "\n",
    "i = 1\n",
    "\n",
    "for result in results:\n",
    "    if i <= 3:\n",
    "        i += 1\n",
    "        print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is also capable of reading a SQL table directly into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with local_engine.connect() as cnx:\n",
    "    df = pd.read_sql_table('spatial_ref_sys', cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the table\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display the table\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# row and column count\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show stats on numeric columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show stats on df columns, types, and indexes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's clean up\n",
    "del SRS, query, result, df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI  🐝º¤ø,¸¸,ø¤º°'°º¤ø,¸.\n",
    "\n",
    "Artificial Intelligence, Machine Learning, Natural Language Processing, Large Language Models, and other facets of advanced business intelligence are exciting not because they're buzz words, but largely because very smart people have been fantasizing these technologies for a very long time and we are finally entering a stage in their lifecycle where they are capable of providing practical benefits, as well as becoming effective tools for developing similar tools to themselves increasing the rate of innovation substantially.\n",
    "\n",
    "One of the keys to effectively leveraging these technological advancements is to be rational and pragmatic about their applications, strengths, and weaknesses.\n",
    "\n",
    "It should be explicitly (and strongly) noted that most AI tools are not considered \"production ready\", have very serious (and largely unanswered) ethical implications, various privacy concerns, and most of all are generally experimental and prone to being glitchy and very often just plain wrong. They are also usually trained on very large low quality datasets, so in practically all cases YMMV.\n",
    "\n",
    "That said, AI tools have seen extraordinary growth in recent times and are driving a significant paradigm shift in the way we work with and interact with technology. One thing they are terrific for are for producing quick and cheap to produce exploratory outputs to simple business questions, which can uncover a lot of value and provide the anecdotal evidence needed to warrant an discovery exercise and more formal proposal.\n",
    "\n",
    "One of the most effective ways of realizing these benefits is using the various tools available to provide a conversational model which is context aware and trained (or provided background context) on a dataset to be evaluated so that the model can be asked questions about your business.\n",
    "\n",
    "We've collected a few of these in this stack to get an intelligent data science ready environment up that is simple to use and configure. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚠ Personal Privacy considerations with AI Tools\n",
    "\n",
    "One key consideration when utilizing AI tools is the appropriate licensing and utilization of data on the platform, as well as the exposure of private or sensitive data to such tools.\n",
    "\n",
    "Personal Identifiable Information (PII) is protected by law in many jurisdictions, and exposure of such information to AI platforms may constitute an infraction with significant repercussions, or even a TOS violation on the platform itself.\n",
    "\n",
    "Although this is largely case specific, it is possible (for the most part) to mitigate this risk by taking into account the following considerations:\n",
    "\n",
    "- Understanding how a *specific* tool or library handles this particular issue is the first step forward\n",
    "- Note that AI platforms may process or store your data and requests, and use it for further training\n",
    "- Retraining models can expose sensitive data that is not PII, such as internal policy IDs\n",
    "- Utilizing the \"schema\" or data structure of a set of records is typically safe as it does not constitute PII\n",
    "- Anonymized or random data is usually safe for use (although effective processing is non-trivial to achieve without leaks or preventing the reverse engineering of PII)\n",
    "- PII encompasses a lot of other data besides SSN, Name, or Email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PandasAI\n",
    "\n",
    "A simple ai tool for pushing contextual information from a dataframe to a LLM with a focus on simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "import pandas as pd\n",
    "import pandasai\n",
    "\n",
    "CONFIG = ConfigParser()\n",
    "CONFIG.read('/nb.cfg')\n",
    "OPENAI_API_KEY = CONFIG['openai']['OPENAI_API_KEY']\n",
    "del CONFIG\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"],\n",
    "    \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064],\n",
    "    \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12]\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasai import PandasAI\n",
    "from pandasai.llm.openai import OpenAI\n",
    "\n",
    "# llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "llm = OpenAI(api_token=OPENAI_API_KEY)\n",
    "# pandasai, hupyter, and docker have some caching issues \n",
    "# https://github.com/gventuri/pandas-ai/issues/269\n",
    "# pandas_ai = PandasAI(llm=llm, enforce_privacy=True, enable_cache=False, verbose=True, conversational=True)  # debug\n",
    "pandas_ai = PandasAI(llm=llm, enforce_privacy=True, enable_cache=False, verbose=False, conversational=True)\n",
    "response = pandas_ai(df, 'Which are the 5 happiest countries?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "df = ''\n",
    "chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)\n",
    "agent = create_pandas_dataframe_agent(chat, df, verbose=True)\n",
    "agent.run('prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lamini-ai.github.io\n",
    "\n",
    "from llama import LLMEngine, Type, Context\n",
    "\n",
    "llm = LLMEngine(\n",
    "    id=\"example_llm\",\n",
    "    config={\"production.key\": \"<YOUR-KEY-HERE>\"}\n",
    "    )\n",
    "\n",
    "class Test(Type):\n",
    "    test_string: str = Context(\"just a test\")\n",
    "\n",
    "llm = LLMEngine(id=\"my_test\")\n",
    "\n",
    "test = Test(test_string=\"testing 123\")\n",
    "llm(test, output_type=Test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
